{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 3, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 79, "column": 0}, "map": {"version":3,"sources":["file:///Users/itwelaibomu/Desktop/code/agents-2025-dope/apps/web/src/services/pineconeService.ts"],"sourcesContent":["import { Pinecone } from '@pinecone-database/pinecone';\nimport dotenv from 'dotenv';\nimport { EmployeeDataInput, TranscriptDataInput } from '../types/metadata';\n\n// Employee data interface based on the metadata types\ninterface EmployeeData {\n  employeeId: string;\n  name: string;\n  organization: string;\n  position: string;\n  reportsTo: string;\n  gender?: string | null;\n  assessmentDate: string;\n  all34: string[];\n  leadDomain: 'Executing' | 'Influencing' | 'Relationship Building' | 'Strategic Thinking';\n  themeDomains: {\n    Executing: string[];\n    Influencing: string[];\n    RelationshipBuilding: string[];\n    StrategyThinking: string[];\n  };\n  bestCollabWith: string;\n  communicationTips: string;\n  howToCoach: string;\n  motivators: string[];\n  demotivators: string[];\n  watchouts: string;\n  evidenceQuotes: Array<{\n    quote: string;\n    section: string;\n  }>;\n  sourceDocUrl?: string | null;\n  sourceProvenance?: string | null;\n}\n\ndotenv.config();\n\n// Get API key from environment variables\nexport const getApiKey = () => {\n  const apiKey = process.env.NODE_ENV === 'production' ? process.env.PINECONE_API_KEY : process.env.NEXT_PUBLIC_PINECONE_API_KEY;\n\n  return apiKey || '';\n};\n\nconst pc = new Pinecone({\n  apiKey: getApiKey()\n});\n\nexport { pc };\n\n// Create a new index with embeddings model\nexport async function createIndex(indexName: string) {\n  try {\n    await pc.createIndexForModel({\n      name: indexName,\n      cloud: 'aws',\n      region: 'us-east-1',\n      embed: {\n        model: 'llama-text-embed-v2',\n        fieldMap: { text: 'chunk_text' },\n      },\n      waitUntilReady: true,\n    });\n    return { success: true };\n  } catch (error) {\n    console.error('Error creating index:', error);\n    throw new Error(`Failed to create index: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n// Helper function to estimate metadata size in bytes\nfunction estimateMetadataSize(metadata: Record<string, any>): number {\n  return JSON.stringify(metadata).length * 2; // Rough estimate (UTF-16)\n}\n\n// Helper function to split large metadata into smaller chunks\nfunction splitMetadata(metadata: Record<string, any>, maxSizeBytes: number = 30000): Record<string, any>[] {\n  const metadataStr = JSON.stringify(metadata);\n  const estimatedSize = metadataStr.length * 2;\n\n  if (estimatedSize <= maxSizeBytes) {\n    return [metadata];\n  }\n\n  // Split large arrays and strings\n  const chunks: Record<string, any>[] = [];\n  const baseMetadata = { ...metadata };\n\n  // Handle large array fields\n  const arrayFields = ['participants', 'action_items', 'concepts_discussed', 'key_topics'];\n\n  for (const field of arrayFields) {\n    if (Array.isArray(baseMetadata[field]) && baseMetadata[field].length > 10) {\n      const items = baseMetadata[field];\n      const chunkSize = Math.ceil(items.length / Math.ceil(estimatedSize / maxSizeBytes));\n\n      for (let i = 0; i < items.length; i += chunkSize) {\n        const chunk = {\n          ...baseMetadata,\n          [field]: items.slice(i, i + chunkSize),\n          chunk_index: Math.floor(i / chunkSize),\n          total_chunks: Math.ceil(items.length / chunkSize)\n        };\n        chunks.push(chunk);\n      }\n\n      // Remove the original field from base metadata\n      delete baseMetadata[field];\n    }\n  }\n\n  // If no chunks were created, create one with reduced metadata\n  if (chunks.length === 0) {\n    const reducedMetadata = { ...baseMetadata };\n    // Remove or truncate large string fields\n    if (reducedMetadata.summary && reducedMetadata.summary.length > 1000) {\n      reducedMetadata.summary = reducedMetadata.summary.substring(0, 1000) + '...';\n    }\n    chunks.push(reducedMetadata);\n  }\n\n  return chunks;\n}\n\n// Helper function to chunk large content\nfunction chunkContent(content: string, maxChunkSize: number = 2000): string[] {\n  if (content.length <= maxChunkSize) {\n    return [content];\n  }\n\n  const chunks: string[] = [];\n  const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  let currentChunk = '';\n\n  for (const sentence of sentences) {\n    const trimmedSentence = sentence.trim();\n    if (currentChunk.length + trimmedSentence.length + 1 <= maxChunkSize) {\n      currentChunk += (currentChunk ? '. ' : '') + trimmedSentence;\n    } else {\n      if (currentChunk) {\n        chunks.push(currentChunk + '.');\n      }\n      currentChunk = trimmedSentence;\n    }\n  }\n\n  if (currentChunk) {\n    chunks.push(currentChunk + '.');\n  }\n\n  return chunks;\n}\n\n// Add data to an index with automatic chunking for large content\nexport async function addToIndex(indexName: string, data: { title: string; content: string; metadata?: Record<string, any> }) {\n  try {\n    const index = pc.index(indexName);\n\n    // Chunk the content if it's too large\n    const contentChunks = chunkContent(data.content);\n\n    // Split metadata if it's too large\n    const metadataChunks = data.metadata ? splitMetadata(data.metadata) : [{}];\n\n    const records = [];\n\n    // Create records for each combination of content and metadata chunks\n    for (let i = 0; i < contentChunks.length; i++) {\n      for (let j = 0; j < metadataChunks.length; j++) {\n        const id = `${Date.now()}-${Math.random().toString(36).substr(2, 9)}-${i}-${j}`;\n\n        const record: any = {\n          _id: id,\n          chunk_text: contentChunks[i],\n          title: data.title,\n          content_chunk_index: i,\n          total_content_chunks: contentChunks.length,\n          ...metadataChunks[j],\n          created_at: new Date().toISOString(),\n        };\n\n        // Check if this record would be too large\n        const estimatedSize = estimateMetadataSize(record);\n        if (estimatedSize > 35000) { // Leave some buffer\n          console.warn(`Record ${id} is still too large (${estimatedSize} bytes), further reducing...`);\n          // Further reduce the record\n          record.chunk_text = record.chunk_text.substring(0, 1000) + '...';\n          if (record.summary) {\n            record.summary = record.summary.substring(0, 500) + '...';\n          }\n        }\n\n        records.push(record);\n      }\n    }\n\n    // Batch upsert records\n    const batchSize = 100; // Pinecone batch limit\n    for (let i = 0; i < records.length; i += batchSize) {\n      const batch = records.slice(i, i + batchSize);\n      await index.upsertRecords(batch);\n    }\n\n    return { success: true, id: records[0]._id, totalChunks: records.length };\n  } catch (error) {\n    console.error('Error adding to index:', error);\n    throw new Error(`Failed to add to index: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n// Specialized function for adding employee data to Pinecone\nexport async function addEmployeeDataToIndex(indexName: string, data: EmployeeDataInput) {\n  try {\n    const index = pc.index(indexName);\n    const { employeeData, tags = [], source } = data;\n\n    // Create searchable content from employee data\n    const searchableContent = createEmployeeSearchableContent(employeeData);\n\n    // Chunk the content if it's too large\n    const contentChunks = chunkContent(searchableContent);\n\n    const records = [];\n\n    // Create records for each content chunk\n    for (let i = 0; i < contentChunks.length; i++) {\n      const id = `${employeeData.employeeId}-${Date.now()}-${i}`;\n\n      const record: any = {\n        _id: id,\n        chunk_text: contentChunks[i],\n        title: `${employeeData.name} - ${employeeData.position}`,\n        content_chunk_index: i,\n        total_content_chunks: contentChunks.length,\n\n        // Employee-specific metadata\n        employeeId: employeeData.employeeId,\n        name: employeeData.name,\n        organization: employeeData.organization,\n        position: employeeData.position,\n        reportsTo: employeeData.reportsTo,\n        gender: employeeData.gender,\n        assessmentDate: employeeData.assessmentDate,\n        leadDomain: employeeData.leadDomain,\n\n        // Arrays as comma-separated strings for better searchability\n        all34: Array.isArray(employeeData.all34) ? employeeData.all34.join(', ') : employeeData.all34,\n        motivators: Array.isArray(employeeData.motivators) ? employeeData.motivators.join(', ') : employeeData.motivators,\n        demotivators: Array.isArray(employeeData.demotivators) ? employeeData.demotivators.join(', ') : employeeData.demotivators,\n\n        // Text fields\n        bestCollabWith: employeeData.bestCollabWith,\n        communicationTips: employeeData.communicationTips,\n        howToCoach: employeeData.howToCoach,\n        watchouts: employeeData.watchouts,\n\n        // Theme domains as JSON strings\n        themeDomains: JSON.stringify(employeeData.themeDomains),\n        evidenceQuotes: JSON.stringify(employeeData.evidenceQuotes),\n\n        // Source information\n        sourceDocUrl: employeeData.sourceDocUrl,\n        sourceProvenance: employeeData.sourceProvenance,\n\n        // Additional metadata\n        tags: Array.isArray(tags) ? tags.join(', ') : tags,\n        source: source,\n        created_at: new Date().toISOString(),\n        data_type: 'employee_profile'\n      };\n\n      // Check if this record would be too large\n      const estimatedSize = estimateMetadataSize(record);\n      if (estimatedSize > 35000) { // Leave some buffer\n        console.warn(`Record ${id} is still too large (${estimatedSize} bytes), further reducing...`);\n        // Further reduce the record\n        record.chunk_text = record.chunk_text.substring(0, 1000) + '...';\n        if (record.bestCollabWith) {\n          record.bestCollabWith = record.bestCollabWith.substring(0, 500) + '...';\n        }\n      }\n\n      records.push(record);\n    }\n\n    // Batch upsert records\n    const batchSize = 100; // Pinecone batch limit\n    for (let i = 0; i < records.length; i += batchSize) {\n      const batch = records.slice(i, i + batchSize);\n      await index.upsertRecords(batch);\n    }\n\n    return { success: true, id: records[0]._id, totalChunks: records.length };\n  } catch (error) {\n    console.error('Error adding employee data to index:', error);\n    throw new Error(`Failed to add employee data to index: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n// Specialized function for adding transcript data to Pinecone\nexport async function addTranscriptDataToIndex(indexName: string, data: TranscriptDataInput) {\n  try {\n    const index = pc.index(indexName);\n    const { transcriptData, tags = [], source } = data;\n\n    const searchableContent = createTranscriptSearchableContent(transcriptData);\n    const contentChunks = chunkContent(searchableContent);\n    const records: any[] = [];\n\n    for (let i = 0; i < contentChunks.length; i++) {\n      const id = `${transcriptData.date || 'unknown-date'}-${Date.now()}-${i}`;\n      const record: any = {\n        _id: id,\n        chunk_text: contentChunks[i],\n        title: `${transcriptData.title || 'Transcript'}${transcriptData.date ? ' - ' + transcriptData.date : ''}`,\n        content_chunk_index: i,\n        total_content_chunks: contentChunks.length,\n\n        meetingType: transcriptData.meetingType,\n        duration: transcriptData.duration,\n        participants: Array.isArray(transcriptData.participants) ? transcriptData.participants.join(', ') : transcriptData.participants,\n        location: transcriptData.location,\n        department: transcriptData.department,\n        confidentialityLevel: transcriptData.confidentialityLevel,\n        action_items: Array.isArray(transcriptData.action_items) ? transcriptData.action_items.join(', ') : transcriptData.action_items,\n        concepts_discussed: Array.isArray(transcriptData.concepts_discussed) ? transcriptData.concepts_discussed.join(', ') : transcriptData.concepts_discussed,\n        date: transcriptData.date,\n        key_topics: Array.isArray(transcriptData.key_topics) ? transcriptData.key_topics.join(', ') : transcriptData.key_topics,\n        summary: transcriptData.summary,\n\n        tags: Array.isArray(tags) ? tags.join(', ') : tags,\n        source: source,\n        created_at: new Date().toISOString(),\n        data_type: 'transcript'\n      };\n\n      const estimatedSize = estimateMetadataSize(record);\n      if (estimatedSize > 35000) {\n        console.warn(`Transcript record ${id} is too large (${estimatedSize} bytes), reducing...`);\n        record.chunk_text = record.chunk_text.substring(0, 1000) + '...';\n        if (record.summary && typeof record.summary === 'string') {\n          record.summary = record.summary.substring(0, 500) + '...';\n        }\n      }\n\n      records.push(record);\n    }\n\n    const batchSize = 100;\n    for (let i = 0; i < records.length; i += batchSize) {\n      const batch = records.slice(i, i + batchSize);\n      await index.upsertRecords(batch);\n    }\n\n    return { success: true, id: records[0]._id, totalChunks: records.length };\n  } catch (error) {\n    console.error('Error adding transcript data to index:', error);\n    throw new Error(`Failed to add transcript data to index: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n// Helper function to create searchable content from employee data\nfunction createEmployeeSearchableContent(employeeData: EmployeeData): string {\n  const strengths = Array.isArray(employeeData.all34) ? employeeData.all34.join(', ') : employeeData.all34;\n  const motivators = Array.isArray(employeeData.motivators) ? employeeData.motivators.join(', ') : employeeData.motivators;\n  const demotivators = Array.isArray(employeeData.demotivators) ? employeeData.demotivators.join(', ') : employeeData.demotivators;\n\n  return `Employee Profile: ${employeeData.name}\nPosition: ${employeeData.position}\nOrganization: ${employeeData.organization}\nReports To: ${employeeData.reportsTo}\nLead Domain: ${employeeData.leadDomain}\nAssessment Date: ${employeeData.assessmentDate}\n\nStrengths: ${strengths}\n\nBest Collaboration: ${employeeData.bestCollabWith}\n\nCommunication Tips: ${employeeData.communicationTips}\n\nHow to Coach: ${employeeData.howToCoach}\n\nMotivators: ${motivators}\n\nDemotivators: ${demotivators}\n\nWatchouts: ${employeeData.watchouts}\n\nTheme Domains: ${JSON.stringify(employeeData.themeDomains)}\n\nEvidence Quotes: ${JSON.stringify(employeeData.evidenceQuotes)}`;\n}\n\nfunction createTranscriptSearchableContent(transcriptData: import('../types/metadata').TranscriptMetadata): string {\n  const participants = Array.isArray(transcriptData.participants) ? transcriptData.participants.join(', ') : transcriptData.participants;\n  const actionItems = Array.isArray(transcriptData.action_items) ? transcriptData.action_items.join('; ') : transcriptData.action_items;\n  const conceptsDiscussed = Array.isArray(transcriptData.concepts_discussed) ? transcriptData.concepts_discussed.join('; ') : transcriptData.concepts_discussed;\n  const keyTopics = Array.isArray(transcriptData.key_topics) ? transcriptData.key_topics.join('; ') : transcriptData.key_topics;\n\n  return `Transcript Summary\nMeeting Type: ${transcriptData.meetingType || 'N/A'}\nDate: ${transcriptData.date || 'N/A'}\nDuration: ${typeof transcriptData.duration === 'number' ? `${transcriptData.duration} min` : 'N/A'}\nParticipants: ${participants || 'N/A'}\nLocation: ${transcriptData.location || 'N/A'}\nDepartment: ${transcriptData.department || 'N/A'}\nConfidentiality: ${transcriptData.confidentialityLevel || 'N/A'}\n\nKey Topics: ${keyTopics || 'N/A'}\nConcepts Discussed: ${conceptsDiscussed || 'N/A'}\nAction Items: ${actionItems || 'N/A'}\n\nSummary: ${transcriptData.summary || 'N/A'}`;\n}\n\n// List existing indexes\nexport async function listIndexesFromPinecone() {\n  try {\n    const indexes = await pc.listIndexes();\n    return indexes;\n  } catch (error) {\n    console.error('Error listing indexes:', error);\n    throw new Error(`Failed to list indexes: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n\n// Semantic search using query text on an index with integrated embeddings\nexport async function semanticSearch(\n  indexName: string,\n  params: { query: string; topK?: number; namespace?: string; fields?: string[] }\n) {\n  try {\n\n    const index = pc.index(indexName);\n    const targetNamespace = (params.namespace && params.namespace.length > 0)\n      ? params.namespace\n      : '__default__';\n    const namespace = index.namespace(targetNamespace);\n\n    const response = await namespace.searchRecords({\n      query: {\n        topK: params.topK ?? 5,\n        inputs: { text: params.query },\n      },\n      // If fields are not specified, Pinecone returns all fields\n      fields: params.fields && params.fields.length > 0 ? params.fields : undefined,\n    });\n\n    return response;\n  } catch (error) {\n    console.error('Error performing semantic search:', error);\n    throw new Error(`Failed to perform semantic search: ${error instanceof Error ? error.message : 'Unknown error'}`);\n  }\n}\n\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAAA;AACA;;;AAkCA,kJAAM,CAAC,MAAM;AAGN,MAAM,YAAY;IACvB,MAAM,SAAS,sCAAwC;IAEvD,OAAO,UAAU;AACnB;AAEA,MAAM,KAAK,IAAI,iLAAQ,CAAC;IACtB,QAAQ;AACV;;AAKO,eAAe,YAAY,SAAiB;IACjD,IAAI;QACF,MAAM,GAAG,mBAAmB,CAAC;YAC3B,MAAM;YACN,OAAO;YACP,QAAQ;YACR,OAAO;gBACL,OAAO;gBACP,UAAU;oBAAE,MAAM;gBAAa;YACjC;YACA,gBAAgB;QAClB;QACA,OAAO;YAAE,SAAS;QAAK;IACzB,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,yBAAyB;QACvC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IACvG;AACF;AAEA,qDAAqD;AACrD,SAAS,qBAAqB,QAA6B;IACzD,OAAO,KAAK,SAAS,CAAC,UAAU,MAAM,GAAG,GAAG,0BAA0B;AACxE;AAEA,8DAA8D;AAC9D,SAAS,cAAc,QAA6B,EAAE,eAAuB,KAAK;IAChF,MAAM,cAAc,KAAK,SAAS,CAAC;IACnC,MAAM,gBAAgB,YAAY,MAAM,GAAG;IAE3C,IAAI,iBAAiB,cAAc;QACjC,OAAO;YAAC;SAAS;IACnB;IAEA,iCAAiC;IACjC,MAAM,SAAgC,EAAE;IACxC,MAAM,eAAe;QAAE,GAAG,QAAQ;IAAC;IAEnC,4BAA4B;IAC5B,MAAM,cAAc;QAAC;QAAgB;QAAgB;QAAsB;KAAa;IAExF,KAAK,MAAM,SAAS,YAAa;QAC/B,IAAI,MAAM,OAAO,CAAC,YAAY,CAAC,MAAM,KAAK,YAAY,CAAC,MAAM,CAAC,MAAM,GAAG,IAAI;YACzE,MAAM,QAAQ,YAAY,CAAC,MAAM;YACjC,MAAM,YAAY,KAAK,IAAI,CAAC,MAAM,MAAM,GAAG,KAAK,IAAI,CAAC,gBAAgB;YAErE,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAM,EAAE,KAAK,UAAW;gBAChD,MAAM,QAAQ;oBACZ,GAAG,YAAY;oBACf,CAAC,MAAM,EAAE,MAAM,KAAK,CAAC,GAAG,IAAI;oBAC5B,aAAa,KAAK,KAAK,CAAC,IAAI;oBAC5B,cAAc,KAAK,IAAI,CAAC,MAAM,MAAM,GAAG;gBACzC;gBACA,OAAO,IAAI,CAAC;YACd;YAEA,+CAA+C;YAC/C,OAAO,YAAY,CAAC,MAAM;QAC5B;IACF;IAEA,8DAA8D;IAC9D,IAAI,OAAO,MAAM,KAAK,GAAG;QACvB,MAAM,kBAAkB;YAAE,GAAG,YAAY;QAAC;QAC1C,yCAAyC;QACzC,IAAI,gBAAgB,OAAO,IAAI,gBAAgB,OAAO,CAAC,MAAM,GAAG,MAAM;YACpE,gBAAgB,OAAO,GAAG,gBAAgB,OAAO,CAAC,SAAS,CAAC,GAAG,QAAQ;QACzE;QACA,OAAO,IAAI,CAAC;IACd;IAEA,OAAO;AACT;AAEA,yCAAyC;AACzC,SAAS,aAAa,OAAe,EAAE,eAAuB,IAAI;IAChE,IAAI,QAAQ,MAAM,IAAI,cAAc;QAClC,OAAO;YAAC;SAAQ;IAClB;IAEA,MAAM,SAAmB,EAAE;IAC3B,MAAM,YAAY,QAAQ,KAAK,CAAC,UAAU,MAAM,CAAC,CAAA,IAAK,EAAE,IAAI,GAAG,MAAM,GAAG;IACxE,IAAI,eAAe;IAEnB,KAAK,MAAM,YAAY,UAAW;QAChC,MAAM,kBAAkB,SAAS,IAAI;QACrC,IAAI,aAAa,MAAM,GAAG,gBAAgB,MAAM,GAAG,KAAK,cAAc;YACpE,gBAAgB,CAAC,eAAe,OAAO,EAAE,IAAI;QAC/C,OAAO;YACL,IAAI,cAAc;gBAChB,OAAO,IAAI,CAAC,eAAe;YAC7B;YACA,eAAe;QACjB;IACF;IAEA,IAAI,cAAc;QAChB,OAAO,IAAI,CAAC,eAAe;IAC7B;IAEA,OAAO;AACT;AAGO,eAAe,WAAW,SAAiB,EAAE,IAAwE;IAC1H,IAAI;QACF,MAAM,QAAQ,GAAG,KAAK,CAAC;QAEvB,sCAAsC;QACtC,MAAM,gBAAgB,aAAa,KAAK,OAAO;QAE/C,mCAAmC;QACnC,MAAM,iBAAiB,KAAK,QAAQ,GAAG,cAAc,KAAK,QAAQ,IAAI;YAAC,CAAC;SAAE;QAE1E,MAAM,UAAU,EAAE;QAElB,qEAAqE;QACrE,IAAK,IAAI,IAAI,GAAG,IAAI,cAAc,MAAM,EAAE,IAAK;YAC7C,IAAK,IAAI,IAAI,GAAG,IAAI,eAAe,MAAM,EAAE,IAAK;gBAC9C,MAAM,KAAK,GAAG,KAAK,GAAG,GAAG,CAAC,EAAE,KAAK,MAAM,GAAG,QAAQ,CAAC,IAAI,MAAM,CAAC,GAAG,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE,GAAG;gBAE/E,MAAM,SAAc;oBAClB,KAAK;oBACL,YAAY,aAAa,CAAC,EAAE;oBAC5B,OAAO,KAAK,KAAK;oBACjB,qBAAqB;oBACrB,sBAAsB,cAAc,MAAM;oBAC1C,GAAG,cAAc,CAAC,EAAE;oBACpB,YAAY,IAAI,OAAO,WAAW;gBACpC;gBAEA,0CAA0C;gBAC1C,MAAM,gBAAgB,qBAAqB;gBAC3C,IAAI,gBAAgB,OAAO;oBACzB,QAAQ,IAAI,CAAC,CAAC,OAAO,EAAE,GAAG,qBAAqB,EAAE,cAAc,4BAA4B,CAAC;oBAC5F,4BAA4B;oBAC5B,OAAO,UAAU,GAAG,OAAO,UAAU,CAAC,SAAS,CAAC,GAAG,QAAQ;oBAC3D,IAAI,OAAO,OAAO,EAAE;wBAClB,OAAO,OAAO,GAAG,OAAO,OAAO,CAAC,SAAS,CAAC,GAAG,OAAO;oBACtD;gBACF;gBAEA,QAAQ,IAAI,CAAC;YACf;QACF;QAEA,uBAAuB;QACvB,MAAM,YAAY,KAAK,uBAAuB;QAC9C,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,KAAK,UAAW;YAClD,MAAM,QAAQ,QAAQ,KAAK,CAAC,GAAG,IAAI;YACnC,MAAM,MAAM,aAAa,CAAC;QAC5B;QAEA,OAAO;YAAE,SAAS;YAAM,IAAI,OAAO,CAAC,EAAE,CAAC,GAAG;YAAE,aAAa,QAAQ,MAAM;QAAC;IAC1E,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,0BAA0B;QACxC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IACvG;AACF;AAGO,eAAe,uBAAuB,SAAiB,EAAE,IAAuB;IACrF,IAAI;QACF,MAAM,QAAQ,GAAG,KAAK,CAAC;QACvB,MAAM,EAAE,YAAY,EAAE,OAAO,EAAE,EAAE,MAAM,EAAE,GAAG;QAE5C,+CAA+C;QAC/C,MAAM,oBAAoB,gCAAgC;QAE1D,sCAAsC;QACtC,MAAM,gBAAgB,aAAa;QAEnC,MAAM,UAAU,EAAE;QAElB,wCAAwC;QACxC,IAAK,IAAI,IAAI,GAAG,IAAI,cAAc,MAAM,EAAE,IAAK;YAC7C,MAAM,KAAK,GAAG,aAAa,UAAU,CAAC,CAAC,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,GAAG;YAE1D,MAAM,SAAc;gBAClB,KAAK;gBACL,YAAY,aAAa,CAAC,EAAE;gBAC5B,OAAO,GAAG,aAAa,IAAI,CAAC,GAAG,EAAE,aAAa,QAAQ,EAAE;gBACxD,qBAAqB;gBACrB,sBAAsB,cAAc,MAAM;gBAE1C,6BAA6B;gBAC7B,YAAY,aAAa,UAAU;gBACnC,MAAM,aAAa,IAAI;gBACvB,cAAc,aAAa,YAAY;gBACvC,UAAU,aAAa,QAAQ;gBAC/B,WAAW,aAAa,SAAS;gBACjC,QAAQ,aAAa,MAAM;gBAC3B,gBAAgB,aAAa,cAAc;gBAC3C,YAAY,aAAa,UAAU;gBAEnC,6DAA6D;gBAC7D,OAAO,MAAM,OAAO,CAAC,aAAa,KAAK,IAAI,aAAa,KAAK,CAAC,IAAI,CAAC,QAAQ,aAAa,KAAK;gBAC7F,YAAY,MAAM,OAAO,CAAC,aAAa,UAAU,IAAI,aAAa,UAAU,CAAC,IAAI,CAAC,QAAQ,aAAa,UAAU;gBACjH,cAAc,MAAM,OAAO,CAAC,aAAa,YAAY,IAAI,aAAa,YAAY,CAAC,IAAI,CAAC,QAAQ,aAAa,YAAY;gBAEzH,cAAc;gBACd,gBAAgB,aAAa,cAAc;gBAC3C,mBAAmB,aAAa,iBAAiB;gBACjD,YAAY,aAAa,UAAU;gBACnC,WAAW,aAAa,SAAS;gBAEjC,gCAAgC;gBAChC,cAAc,KAAK,SAAS,CAAC,aAAa,YAAY;gBACtD,gBAAgB,KAAK,SAAS,CAAC,aAAa,cAAc;gBAE1D,qBAAqB;gBACrB,cAAc,aAAa,YAAY;gBACvC,kBAAkB,aAAa,gBAAgB;gBAE/C,sBAAsB;gBACtB,MAAM,MAAM,OAAO,CAAC,QAAQ,KAAK,IAAI,CAAC,QAAQ;gBAC9C,QAAQ;gBACR,YAAY,IAAI,OAAO,WAAW;gBAClC,WAAW;YACb;YAEA,0CAA0C;YAC1C,MAAM,gBAAgB,qBAAqB;YAC3C,IAAI,gBAAgB,OAAO;gBACzB,QAAQ,IAAI,CAAC,CAAC,OAAO,EAAE,GAAG,qBAAqB,EAAE,cAAc,4BAA4B,CAAC;gBAC5F,4BAA4B;gBAC5B,OAAO,UAAU,GAAG,OAAO,UAAU,CAAC,SAAS,CAAC,GAAG,QAAQ;gBAC3D,IAAI,OAAO,cAAc,EAAE;oBACzB,OAAO,cAAc,GAAG,OAAO,cAAc,CAAC,SAAS,CAAC,GAAG,OAAO;gBACpE;YACF;YAEA,QAAQ,IAAI,CAAC;QACf;QAEA,uBAAuB;QACvB,MAAM,YAAY,KAAK,uBAAuB;QAC9C,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,KAAK,UAAW;YAClD,MAAM,QAAQ,QAAQ,KAAK,CAAC,GAAG,IAAI;YACnC,MAAM,MAAM,aAAa,CAAC;QAC5B;QAEA,OAAO;YAAE,SAAS;YAAM,IAAI,OAAO,CAAC,EAAE,CAAC,GAAG;YAAE,aAAa,QAAQ,MAAM;QAAC;IAC1E,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,wCAAwC;QACtD,MAAM,IAAI,MAAM,CAAC,sCAAsC,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IACrH;AACF;AAGO,eAAe,yBAAyB,SAAiB,EAAE,IAAyB;IACzF,IAAI;QACF,MAAM,QAAQ,GAAG,KAAK,CAAC;QACvB,MAAM,EAAE,cAAc,EAAE,OAAO,EAAE,EAAE,MAAM,EAAE,GAAG;QAE9C,MAAM,oBAAoB,kCAAkC;QAC5D,MAAM,gBAAgB,aAAa;QACnC,MAAM,UAAiB,EAAE;QAEzB,IAAK,IAAI,IAAI,GAAG,IAAI,cAAc,MAAM,EAAE,IAAK;YAC7C,MAAM,KAAK,GAAG,eAAe,IAAI,IAAI,eAAe,CAAC,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,GAAG;YACxE,MAAM,SAAc;gBAClB,KAAK;gBACL,YAAY,aAAa,CAAC,EAAE;gBAC5B,OAAO,GAAG,eAAe,KAAK,IAAI,eAAe,eAAe,IAAI,GAAG,QAAQ,eAAe,IAAI,GAAG,IAAI;gBACzG,qBAAqB;gBACrB,sBAAsB,cAAc,MAAM;gBAE1C,aAAa,eAAe,WAAW;gBACvC,UAAU,eAAe,QAAQ;gBACjC,cAAc,MAAM,OAAO,CAAC,eAAe,YAAY,IAAI,eAAe,YAAY,CAAC,IAAI,CAAC,QAAQ,eAAe,YAAY;gBAC/H,UAAU,eAAe,QAAQ;gBACjC,YAAY,eAAe,UAAU;gBACrC,sBAAsB,eAAe,oBAAoB;gBACzD,cAAc,MAAM,OAAO,CAAC,eAAe,YAAY,IAAI,eAAe,YAAY,CAAC,IAAI,CAAC,QAAQ,eAAe,YAAY;gBAC/H,oBAAoB,MAAM,OAAO,CAAC,eAAe,kBAAkB,IAAI,eAAe,kBAAkB,CAAC,IAAI,CAAC,QAAQ,eAAe,kBAAkB;gBACvJ,MAAM,eAAe,IAAI;gBACzB,YAAY,MAAM,OAAO,CAAC,eAAe,UAAU,IAAI,eAAe,UAAU,CAAC,IAAI,CAAC,QAAQ,eAAe,UAAU;gBACvH,SAAS,eAAe,OAAO;gBAE/B,MAAM,MAAM,OAAO,CAAC,QAAQ,KAAK,IAAI,CAAC,QAAQ;gBAC9C,QAAQ;gBACR,YAAY,IAAI,OAAO,WAAW;gBAClC,WAAW;YACb;YAEA,MAAM,gBAAgB,qBAAqB;YAC3C,IAAI,gBAAgB,OAAO;gBACzB,QAAQ,IAAI,CAAC,CAAC,kBAAkB,EAAE,GAAG,eAAe,EAAE,cAAc,oBAAoB,CAAC;gBACzF,OAAO,UAAU,GAAG,OAAO,UAAU,CAAC,SAAS,CAAC,GAAG,QAAQ;gBAC3D,IAAI,OAAO,OAAO,IAAI,OAAO,OAAO,OAAO,KAAK,UAAU;oBACxD,OAAO,OAAO,GAAG,OAAO,OAAO,CAAC,SAAS,CAAC,GAAG,OAAO;gBACtD;YACF;YAEA,QAAQ,IAAI,CAAC;QACf;QAEA,MAAM,YAAY;QAClB,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,KAAK,UAAW;YAClD,MAAM,QAAQ,QAAQ,KAAK,CAAC,GAAG,IAAI;YACnC,MAAM,MAAM,aAAa,CAAC;QAC5B;QAEA,OAAO;YAAE,SAAS;YAAM,IAAI,OAAO,CAAC,EAAE,CAAC,GAAG;YAAE,aAAa,QAAQ,MAAM;QAAC;IAC1E,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,0CAA0C;QACxD,MAAM,IAAI,MAAM,CAAC,wCAAwC,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IACvH;AACF;AAEA,kEAAkE;AAClE,SAAS,gCAAgC,YAA0B;IACjE,MAAM,YAAY,MAAM,OAAO,CAAC,aAAa,KAAK,IAAI,aAAa,KAAK,CAAC,IAAI,CAAC,QAAQ,aAAa,KAAK;IACxG,MAAM,aAAa,MAAM,OAAO,CAAC,aAAa,UAAU,IAAI,aAAa,UAAU,CAAC,IAAI,CAAC,QAAQ,aAAa,UAAU;IACxH,MAAM,eAAe,MAAM,OAAO,CAAC,aAAa,YAAY,IAAI,aAAa,YAAY,CAAC,IAAI,CAAC,QAAQ,aAAa,YAAY;IAEhI,OAAO,CAAC,kBAAkB,EAAE,aAAa,IAAI,CAAC;UACtC,EAAE,aAAa,QAAQ,CAAC;cACpB,EAAE,aAAa,YAAY,CAAC;YAC9B,EAAE,aAAa,SAAS,CAAC;aACxB,EAAE,aAAa,UAAU,CAAC;iBACtB,EAAE,aAAa,cAAc,CAAC;;WAEpC,EAAE,UAAU;;oBAEH,EAAE,aAAa,cAAc,CAAC;;oBAE9B,EAAE,aAAa,iBAAiB,CAAC;;cAEvC,EAAE,aAAa,UAAU,CAAC;;YAE5B,EAAE,WAAW;;cAEX,EAAE,aAAa;;WAElB,EAAE,aAAa,SAAS,CAAC;;eAErB,EAAE,KAAK,SAAS,CAAC,aAAa,YAAY,EAAE;;iBAE1C,EAAE,KAAK,SAAS,CAAC,aAAa,cAAc,GAAG;AAChE;AAEA,SAAS,kCAAkC,cAA8D;IACvG,MAAM,eAAe,MAAM,OAAO,CAAC,eAAe,YAAY,IAAI,eAAe,YAAY,CAAC,IAAI,CAAC,QAAQ,eAAe,YAAY;IACtI,MAAM,cAAc,MAAM,OAAO,CAAC,eAAe,YAAY,IAAI,eAAe,YAAY,CAAC,IAAI,CAAC,QAAQ,eAAe,YAAY;IACrI,MAAM,oBAAoB,MAAM,OAAO,CAAC,eAAe,kBAAkB,IAAI,eAAe,kBAAkB,CAAC,IAAI,CAAC,QAAQ,eAAe,kBAAkB;IAC7J,MAAM,YAAY,MAAM,OAAO,CAAC,eAAe,UAAU,IAAI,eAAe,UAAU,CAAC,IAAI,CAAC,QAAQ,eAAe,UAAU;IAE7H,OAAO,CAAC;cACI,EAAE,eAAe,WAAW,IAAI,MAAM;MAC9C,EAAE,eAAe,IAAI,IAAI,MAAM;UAC3B,EAAE,OAAO,eAAe,QAAQ,KAAK,WAAW,GAAG,eAAe,QAAQ,CAAC,IAAI,CAAC,GAAG,MAAM;cACrF,EAAE,gBAAgB,MAAM;UAC5B,EAAE,eAAe,QAAQ,IAAI,MAAM;YACjC,EAAE,eAAe,UAAU,IAAI,MAAM;iBAChC,EAAE,eAAe,oBAAoB,IAAI,MAAM;;YAEpD,EAAE,aAAa,MAAM;oBACb,EAAE,qBAAqB,MAAM;cACnC,EAAE,eAAe,MAAM;;SAE5B,EAAE,eAAe,OAAO,IAAI,OAAO;AAC5C;AAGO,eAAe;IACpB,IAAI;QACF,MAAM,UAAU,MAAM,GAAG,WAAW;QACpC,OAAO;IACT,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,0BAA0B;QACxC,MAAM,IAAI,MAAM,CAAC,wBAAwB,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IACvG;AACF;AAIO,eAAe,eACpB,SAAiB,EACjB,MAA+E;IAE/E,IAAI;QAEF,MAAM,QAAQ,GAAG,KAAK,CAAC;QACvB,MAAM,kBAAkB,AAAC,OAAO,SAAS,IAAI,OAAO,SAAS,CAAC,MAAM,GAAG,IACnE,OAAO,SAAS,GAChB;QACJ,MAAM,YAAY,MAAM,SAAS,CAAC;QAElC,MAAM,WAAW,MAAM,UAAU,aAAa,CAAC;YAC7C,OAAO;gBACL,MAAM,OAAO,IAAI,IAAI;gBACrB,QAAQ;oBAAE,MAAM,OAAO,KAAK;gBAAC;YAC/B;YACA,2DAA2D;YAC3D,QAAQ,OAAO,MAAM,IAAI,OAAO,MAAM,CAAC,MAAM,GAAG,IAAI,OAAO,MAAM,GAAG;QACtE;QAEA,OAAO;IACT,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qCAAqC;QACnD,MAAM,IAAI,MAAM,CAAC,mCAAmC,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,iBAAiB;IAClH;AACF","debugId":null}},
    {"offset": {"line": 481, "column": 0}, "map": {"version":3,"sources":["file:///Users/itwelaibomu/Desktop/code/agents-2025-dope/apps/web/src/app/api/knowledgebase/list-indexes/route.ts"],"sourcesContent":["import { NextResponse } from 'next/server';\nimport { Pinecone } from '@pinecone-database/pinecone';\nimport { getApiKey } from 'apps/web/src/services/pineconeService';\n\n\n// Create a new Pinecone instance\nconst pc = new Pinecone({\n  apiKey: getApiKey()\n});\n\nexport async function GET() {\n  try {\n\n    // List existing indexes\n    const indexes = await pc.listIndexes();\n    \n    return NextResponse.json({ \n      success: true, \n      indexes: indexes.indexes || []\n    });\n  } catch (error) {\n    console.error('Error listing indexes:', error);\n    return NextResponse.json(\n      { error: error instanceof Error ? error.message : 'Failed to list indexes' },\n      { status: 500 }\n    );\n  }\n}\n\n"],"names":[],"mappings":";;;;AAAA;AACA;AACA;;;;AAGA,iCAAiC;AACjC,MAAM,KAAK,IAAI,iLAAQ,CAAC;IACtB,QAAQ,IAAA,gKAAS;AACnB;AAEO,eAAe;IACpB,IAAI;QAEF,wBAAwB;QACxB,MAAM,UAAU,MAAM,GAAG,WAAW;QAEpC,OAAO,gJAAY,CAAC,IAAI,CAAC;YACvB,SAAS;YACT,SAAS,QAAQ,OAAO,IAAI,EAAE;QAChC;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,0BAA0B;QACxC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG;QAAyB,GAC3E;YAAE,QAAQ;QAAI;IAElB;AACF","debugId":null}}]
}